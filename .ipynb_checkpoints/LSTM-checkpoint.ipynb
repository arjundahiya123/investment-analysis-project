{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc3e92-acbb-49db-b5cc-ddefe7177a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Import libraries: numpy (np), pandas (pd)\n",
    "# skLearn is a machine learning library\n",
    "    # from sklearn.preprocessing import MinMaxScaler\n",
    "    # from sklearn.model_selection import train_test_split\n",
    "    # from tensorflow.keras.models import Sequential\n",
    "    # from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1fb022-f712-4091-be7e-f2ada5aa207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Importing stock data using AlphaVantage API\n",
    "\n",
    "\n",
    "#Install AlphaVantage and import libraries if not already imported\n",
    "# !pip install alpha_vantage\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "\n",
    "#Free API key\n",
    "key = 'UO4WWNLT3XIKJ6EI'\n",
    "\n",
    "# Create TimeSeries object\n",
    "ts = TimeSeries(key, output_format='pandas') #requires input of key and output format (default is json)\n",
    "\n",
    "# Sample API Call for Tesla\n",
    "#objects come with a data and a meta component (will likely only need data component), require a stock symbol + interval + output size\n",
    "data, meta = ts.get_intraday('TSLA', interval = '1min', outputsize='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7daa9b0-2715-4ec1-830d-2d85bfe81d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Preprocessing on the data\n",
    "# Shift closing prices by one day to create target variable (next day's closing price): stock_data['Next_Close'] = stock_data['Close'].shift(-1)\n",
    "    # This target variable is essentially what we are aiming to predict, if we know the next day's closing price compared to the current price, we can evaluate if the stock is worth purchasing\n",
    "# Select relevant features: features = stock_data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "    # We can add more features that we require if necessary\n",
    "# Drop last row since it will have NaN for the shifted target\n",
    "    # features = features[:-1]\n",
    "    # target = target.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf81d65-4426-4e94-a02b-df473c38210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1: Additional data that could be used in the furture:\n",
    "# Momentum = Open price – Close price\n",
    "# Average price = (Low price + High price) / 2\n",
    "# Range = High price – Low price\n",
    "# OHLC price = (Open price + High price + Low price + Close price)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea6aad1-dddf-425e-80fe-a1ce2c2a667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Normalize the data with min-max scaling\n",
    "# Instantiate the scaler: scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # The feature range specifies range to scale data\n",
    "# Fit and transform the data: scaled_data = scaler.fit_transform(features)\n",
    "    # Computes the min and max of each feature and scales the data\n",
    "# For each feature, min-max scaling scales the values so that the min value is represented as 0, the max as 1, and everything else in between\n",
    "# Later, we may want to transform scaled values back to the original values by doing:\n",
    "    # original_values = scaler.inverse_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eb5095-ba6a-402b-8de9-84f2012eacc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Create sequences for LSTM\n",
    "# Transform time series data into sequences that can be used as input into LSTM model\n",
    "\n",
    "# def create_sequences(data, target, sequence_length):\n",
    "#     sequences = []\n",
    "#     targets = []\n",
    "#     for i in range(len(data) - sequence_length):\n",
    "#         seq = data[i:i+sequence_length]\n",
    "#         target_value = target.iloc[i+sequence_length]\n",
    "#         sequences.append(seq)\n",
    "#         targets.append(target_value)\n",
    "#     return np.array(sequences), np.array(targets)\n",
    "\n",
    "    # data are the features used to predict the target\n",
    "    # target is the target variable (ex. Next day's closing price)\n",
    "    # sequence_length is the number of time steps the model will look back to make a prediction\n",
    "    # initialize the sequences list and targets list to store the sequences and their respective target values\n",
    "    # the for loop loops over the length of the data minus the sequence length\n",
    "        # ensures we have enough data to create sequences of the specified length\n",
    "    # create sequences by extracting a subsequence of length sequence_length starting from index i\n",
    "        #  represents the historical data that the model will use to make a prediction\n",
    "    # extract the target value corresponding to the end of the sequence\n",
    "    # append the sequence to the sequence list and the target value to the targets list\n",
    "    # the function finally returns the sequences and targets as numpy arrays\n",
    "# sequence_length = 10\n",
    "    # adjust length as needed\n",
    "# X_seq, y_seq = create_sequences(scaled_data, target, sequence_length)\n",
    "    # X_seq will contain sequences of length 10 from the scaled_data features we defined earlier\n",
    "    # y_seq contains the corresponding target values\n",
    "\n",
    "# The main purpose of this step is to create sequences that we can use to train the model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dff127-f8ac-442d-b775-dab06f7fb9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: Split the data into training sets and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "    # X_train contains the features (sequences) that will be used for training the model\n",
    "    # X_test contains the features (sequences) that will be used for testing the model\n",
    "    # y_train contains the corresponding target values for the training set\n",
    "    # y_test contains the corresponding target values for the testing set\n",
    "# We will use the test data to analyze the effectiveness of our model after it is trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332a821-8662-4dc3-9973-237caf97a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: Build the model\n",
    "# model = Sequential()\n",
    "    # In Keras, the sequential model is a linear stack of layers, where you can add one layer at a time\n",
    "# model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    # Adds the first LSTM layer\n",
    "    # units is the number of neurons in the layer\n",
    "    # return_sequences set to true ensures that the next layer in the stack receives the full sequence\n",
    "    # input_shape defines shape of the first layer\n",
    "# model.add(LSTM(units=50))\n",
    "    # Adds the second LSTM layer\n",
    "    # last layer in the stack does not require return_sequences set to true as there is no next layer\n",
    "# model.add(Dense(units=1))\n",
    "    # Adds a dense layer for output\n",
    "    # One output unit (neuron)\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    # Adds a dense layer with one output unit\n",
    "    # A fully connected layer to produce the model's output\n",
    "    # 'adam' is a popular optimization algorithm that adapts the learning rates for each parameter\n",
    "    # mean_squared_error is commonly used for regression tasks, where the goal is to minimize the difference between predicted and true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ead3c-496f-4f17-8cb0-37241914426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.1: Possible GRU + LSTM model\n",
    "# model = Sequential()\n",
    "# model.add(GRU(units=20, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "# model.add(LSTM(units=256, return_sequences=True))\n",
    "# model.add(Dense(units=64))\n",
    "# model.add(Dense(units=1))\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# This is based on this paper (https://doi.org/10.1016/j.socl.2020.100009) which used this method for FOREX daytrading\n",
    "# It resulted in a more accurate results than pure LSTM as well as being more resource efficient as LSTM is typically resource hungry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b87b2-010d-423e-99c3-24e28eb6e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8: Train the model with our data\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "    # X_train are the input features, y_train are the target values for the training set\n",
    "    # epochs is the number of times the entire training dataset is passed forward and backward through the neural network\n",
    "    # the training dataset is divided into batches, and the model weights are updated after processing each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2532f8a7-2eb1-4473-b75f-738b146a47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9: Evaluate the model with our test set\n",
    "# loss = model.evaluate(X_test, y_test)\n",
    "    # use the test data \n",
    "    # loss is essentially the mean squared error\n",
    "        # the average squared difference between the predicted values and the true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a733cd-0090-4e15-ad7c-4316db4853dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10: Make predictions on the test set\n",
    "# predictions = model.predict(X_test)\n",
    "    # Each element in the predictions array corresponds to the model's prediction for a specific data point in the test set\n",
    "    # Basically, predictions includes the target values of each sequence\n",
    "\n",
    "# predicted_prices = scaler.inverse_transform(np.reshape(predictions, (predictions.shape[0], 1)))\n",
    "    # Invereses the predictions to the original scale\n",
    "# actual_prices = scaler.inverse_transform(np.reshape(y_test, (y_test.shape[0], 1)))\n",
    "    # Inverse transofmr the actual closing prices to original scale\n",
    "\n",
    "# Compare predicted_prices to actual_prices to see how accurate the model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a9957-c1da-4244-9640-90b5791f4c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11: Dataframe for analysis\n",
    "# result_df = pd.DataFrame({\n",
    "#     'Actual': actual_prices.flatten(),\n",
    "#     'Predicted': predicted_prices.flatten()\n",
    "# })\n",
    "\n",
    "# Display the results\n",
    "    # result_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
